{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulletines Scraper\n",
    "\n",
    "This notebook uses the links previously scraped to retrieve data from each monthly bulletine published by the Italian Ministry of Justice regarding inmates and stores the raw data in `outputs/raw/bulletines.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import links\n",
    "links_df = pd.read_csv('../outputs/clean/bulletines_links.csv')\n",
    "links_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Hey, open up a browser\"\n",
    "playwright = await async_playwright().start()\n",
    "browser = await playwright.firefox.launch()\n",
    "context = await browser.new_context(viewport={'width': 1280, 'height': 800})\n",
    "# Create a new browser window\n",
    "page = await context.new_page()\n",
    "\n",
    "print(\"Opening up the browser...\")\n",
    "\n",
    "data = []\n",
    "headers = [\n",
    "    'Regione di detenzione',\n",
    "    'Sigla Provincia',\n",
    "    'Istituto',\n",
    "    'Tipo istituto',\n",
    "    'Capienza Regolamentare',\n",
    "    'Detenuti presenti - totale',\n",
    "    'Detenuti presenti - donne',\n",
    "    'Detenuti presenti - stranieri',\n",
    "    'Ultimo aggiornamento',\n",
    "    'ID',]\n",
    "\n",
    "for index, row in links_df.iterrows():\n",
    "\n",
    "    url = row['Hyperlink']\n",
    "    id_value = row['ID']\n",
    "    date_value = row['Ultimo aggiornamento']\n",
    "\n",
    "    # Attempt to navigate to the URL. Retry if timeout occurs.\n",
    "    retry = 0\n",
    "    while retry < 5:\n",
    "        try:\n",
    "            print(f\"Going to {url}\")\n",
    "            await page.goto(url, timeout=60000)\n",
    "            await page.wait_for_timeout(5000)\n",
    "            sleep(random.randint(1, 5))\n",
    "\n",
    "            # Scrape the content\n",
    "            content = await page.content()\n",
    "            soup = BeautifulSoup(content, \"html.parser\")\n",
    "            rows = soup.find_all(\"tr\")\n",
    "\n",
    "\n",
    "            for row in rows[2:]:  # Skipping the header rows\n",
    "                cells = row.find_all(\"td\")\n",
    "                row_data = [cell.get_text(separator=\" \").strip() for cell in cells]\n",
    "\n",
    "                # Append the Date and ID values to the row_data\n",
    "                row_data.extend([date_value, id_value])\n",
    "                data.append(row_data)\n",
    "            print(f\"Scraped page id {id_value}\")\n",
    "            print(\"#######\")\n",
    "            sleep(random.randint(1, 5))\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Timeout while loading {e}, retrying...\")\n",
    "\n",
    "            # Close current browser, wait and reopen\n",
    "            print(\"Reinitializing browser...\")\n",
    "            await browser.close()\n",
    "            sleep(random.randint(1,5))\n",
    "            browser = await playwright.firefox.launch()\n",
    "            context = await browser.new_context(viewport={'width': 1280, 'height': 800})\n",
    "            page = await context.new_page()\n",
    "\n",
    "            retry += 1\n",
    "\n",
    "await browser.close()\n",
    "await playwright.stop()\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store df\n",
    "df.to_csv('../outputs/raw/bulletines_raw.csv', index=False, encoding=\"UTF-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
